{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5hX88yficL7",
        "outputId": "85e19af0-735c-42c5-e0d9-fcd58d1c7faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May 24 04:22:05 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone and Install"
      ],
      "metadata": {
        "id": "qWRGGT7Zjjbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** YOLOv9 is very new. At the moment, we recommend using a fork of the main repository. The `detect.py` script contains a bug that prevents inference. This bug is patched in the fork."
      ],
      "metadata": {
        "id": "9WyY-fboBLZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SkalskiP/yolov9.git\n",
        "%cd yolov9\n",
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "id": "pixgo4qnjdoU",
        "outputId": "ef2a4457-ff2b-499e-8bd9-5a5ddc9384d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov9'...\n",
            "remote: Enumerating objects: 325, done.\u001b[K\n",
            "remote: Counting objects: 100% (218/218), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 325 (delta 159), reused 156 (delta 156), pack-reused 107\u001b[K\n",
            "Receiving objects: 100% (325/325), 2.23 MiB | 27.91 MiB/s, done.\n",
            "Resolving deltas: 100% (165/165), done.\n",
            "/content/yolov9\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q roboflow"
      ],
      "metadata": {
        "id": "TPGqlohQqgAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "555ac92f-70f0-4745-bf07-3715b138c5a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/74.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.9/74.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Nj4eck26wqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download model weights"
      ],
      "metadata": {
        "id": "X8oLIkX2l2P0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** In the YOLOv9 paper, versions `yolov9-s` and `yolov9-m` are also mentioned, but the weights for these models are not yet available in the YOLOv9 [repository](https://github.com/WongKinYiu/yolov9)."
      ],
      "metadata": {
        "id": "0FieRuZnB4wH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt\n",
        "!wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt\n",
        "!wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-c.pt\n",
        "!wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-e.pt"
      ],
      "metadata": {
        "id": "h7j3aUE7l1Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la {HOME}/weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au6np1JS8eRB",
        "outputId": "f08da018-ae03-4707-bf40-7214631fd955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 402440\n",
            "drwxr-xr-x 2 root root      4096 May 24 04:23 .\n",
            "drwxr-xr-x 3 root root      4096 May 24 04:23 ..\n",
            "-rw-r--r-- 1 root root  51508261 Feb 18 12:36 gelan-c.pt\n",
            "-rw-r--r-- 1 root root 117203713 Feb 18 12:36 gelan-e.pt\n",
            "-rw-r--r-- 1 root root 103153312 Feb 18 12:36 yolov9-c.pt\n",
            "-rw-r--r-- 1 root root 140217688 Feb 18 12:36 yolov9-e.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** If you want to run inference using your own file as input, simply upload image to Google Colab and update `SOURCE_IMAGE_PATH` with the path leading to your file."
      ],
      "metadata": {
        "id": "xIKNGnN2kcTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** The dataset must be saved inside the `{HOME}/yolov9` directory, otherwise, the training will not succeed."
      ],
      "metadata": {
        "id": "J5yx2GkI2P7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/yolov9"
      ],
      "metadata": {
        "id": "MyLpftfU2Q1U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbfe19f-cdc5-433b-e1b9-4a3a18651bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** In this tutorial, I will use the [football-players-detection](https://universe.roboflow.com/roboflow-jvuqo/football-players-detection-3zvbc) dataset. Feel free to replace it with your dataset in YOLO format or use another dataset available on [Roboflow Universe](https://universe.roboflow.com). Additionally, if you plan to deploy your model to Roboflow after training, make sure you are the owner of the dataset and that no model is associated with the version of the dataset you are going to training on."
      ],
      "metadata": {
        "id": "eosmGt89vMO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"HzMgbTEHSZxH4Afd3txd\")\n",
        "project = rf.workspace(\"rnd-zgb0t\").project(\"yolov9-kwziv\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov9\")"
      ],
      "metadata": {
        "id": "4J3s_2_7p_gn",
        "outputId": "1fac375e-1d9e-43bb-8709-834da826e854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.29)\n",
            "Requirement already satisfied: certifi==2023.7.22 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2023.7.22)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.25.2)\n",
            "Requirement already satisfied: opencv-python-headless==4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.8.0.74)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.4)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.4.27)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.51.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in yolov9-1 to yolov9:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11159/11159 [00:01<00:00, 5656.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to yolov9-1 in yolov9:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 650/650 [00:00<00:00, 7900.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Custom Model"
      ],
      "metadata": {
        "id": "CTbGpF2IsZ24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/yolov9\n",
        "\n",
        "!python train.py \\\n",
        "--batch 8 --epochs 30 --img 400 --device 0 --min-items 0 --close-mosaic 15 \\\n",
        "--data /content/yolov9/yolov9-1/data.yaml \\\n",
        "--weights /content/yolov9/{HOME}/weights/gelan-c.pt \\\n",
        "--cfg /content/yolov9/models/detect/gelan-c.yaml \\\n",
        "--hyp hyp.scratch-high.yaml"
      ],
      "metadata": {
        "id": "N68Bdf4FsMYW",
        "outputId": "44fc2f8d-d534-425c-e4ea-e3acfb862a46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov9\n",
            "2024-05-24 04:40:12.447197: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-24 04:40:12.447253: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-24 04:40:12.448784: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-24 04:40:12.456524: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-24 04:40:13.812828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/yolov9/{HOME}/weights/gelan-c.pt, cfg=/content/yolov9/models/detect/gelan-c.yaml, data=/content/yolov9/yolov9-1/data.yaml, hyp=hyp.scratch-high.yaml, epochs=30, batch_size=8, imgsz=400, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=15, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "YOLOv5 ðŸš€ 1e33dbb Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, dfl=1.5, obj_pw=1.0, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO ðŸš€ in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO ðŸš€ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
            "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  2                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n",
            "  3                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
            "  4                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n",
            "  5                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
            "  6                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
            "  7                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
            "  8                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
            "  9                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]               \n",
            " 10                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 11           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 12                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
            " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 14           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 15                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]      \n",
            " 16                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
            " 17          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
            " 18                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]       \n",
            " 19                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
            " 20           [-1, 9]  1         0  models.common.Concat                    [1]                           \n",
            " 21                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
            " 22      [15, 18, 21]  1   5496808  models.yolo.DDetect                     [8, [256, 512, 512]]          \n",
            "gelan-c summary: 621 layers, 25443240 parameters, 25443224 gradients, 103.2 GFLOPs\n",
            "\n",
            "Transferred 931/937 items from /content/yolov9/{HOME}/weights/gelan-c.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "WARNING âš ï¸ --img-size 400 must be multiple of max stride 32, updating to 416\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 154 weight(decay=0.0), 161 weight(decay=0.0005), 160 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolov9/yolov9-1/train/labels.cache... 233 images, 0 backgrounds, 0 corrupt: 100% 233/233 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolov9/yolov9-1/valid/labels.cache... 46 images, 0 backgrounds, 0 corrupt: 100% 46/46 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Plotting labels to runs/train/exp3/labels.jpg... \n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp3\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       0/29       2.8G      1.571      3.877      1.746         28        416:   0% 0/30 [00:02<?, ?it/s]WARNING âš ï¸ TensorBoard graph visualization failure Sizes of tensors must match except in dimension 1. Expected size 24 but got size 25 for tensor number 1 in the list.\n",
            "       0/29      2.91G      1.682       3.88      1.834          1        416: 100% 30/30 [00:16<00:00,  1.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.64it/s]\n",
            "                   all         46         53      0.188      0.168      0.105     0.0396\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/29      3.17G      1.581      3.125      1.735          6        416: 100% 30/30 [00:09<00:00,  3.03it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.31it/s]\n",
            "                   all         46         53      0.509      0.381      0.326      0.198\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/29      3.17G      1.447      2.791      1.628          7        416: 100% 30/30 [00:10<00:00,  2.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.16it/s]\n",
            "                   all         46         53      0.369      0.502      0.365      0.193\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/29      3.17G      1.484      2.697      1.643          5        416: 100% 30/30 [00:07<00:00,  4.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.61it/s]\n",
            "                   all         46         53      0.442      0.371      0.354      0.181\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/29      3.17G      1.407        2.6      1.604          4        416: 100% 30/30 [00:11<00:00,  2.67it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.43it/s]\n",
            "                   all         46         53      0.437      0.431      0.413       0.22\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/29      3.17G      1.407       2.51      1.596          1        416: 100% 30/30 [00:11<00:00,  2.72it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.61it/s]\n",
            "                   all         46         53      0.302      0.537      0.435      0.224\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/29      3.17G      1.421        2.5        1.6          4        416: 100% 30/30 [00:12<00:00,  2.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.54it/s]\n",
            "                   all         46         53      0.273      0.515      0.438      0.215\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/29      3.17G       1.49      2.533      1.704          3        416: 100% 30/30 [00:07<00:00,  4.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  3.73it/s]\n",
            "                   all         46         53      0.277      0.539      0.326      0.155\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/29      3.17G      1.475      2.344      1.681          4        416: 100% 30/30 [00:07<00:00,  3.85it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.46it/s]\n",
            "                   all         46         53      0.369      0.552      0.552      0.301\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/29      3.17G      1.446      2.304      1.676          2        416: 100% 30/30 [00:07<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.49it/s]\n",
            "                   all         46         53      0.604       0.39      0.362      0.189\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/29      3.17G      1.389      2.254      1.693          2        416: 100% 30/30 [00:10<00:00,  2.84it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.64it/s]\n",
            "                   all         46         53      0.571      0.353      0.334      0.164\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/29      3.17G      1.438      2.139      1.648          4        416: 100% 30/30 [00:08<00:00,  3.61it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  3.17it/s]\n",
            "                   all         46         53       0.17      0.394       0.29      0.141\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/29      3.17G      1.393      2.053      1.613          4        416: 100% 30/30 [00:07<00:00,  4.16it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.71it/s]\n",
            "                   all         46         53      0.369      0.443      0.386      0.213\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/29      3.17G      1.386      2.166      1.642          2        416: 100% 30/30 [00:11<00:00,  2.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.71it/s]\n",
            "                   all         46         53      0.344      0.515      0.319       0.15\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/29      3.17G      1.431      1.957      1.632          3        416: 100% 30/30 [00:07<00:00,  4.10it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  3.49it/s]\n",
            "                   all         46         53      0.444      0.493      0.493      0.253\n",
            "Closing dataloader mosaic\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/29      3.17G      1.428      2.211      1.708          4        416: 100% 30/30 [00:07<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.68it/s]\n",
            "                   all         46         53      0.646      0.591      0.579      0.299\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/29      3.17G      1.365      2.016      1.709          1        416: 100% 30/30 [00:06<00:00,  4.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.56it/s]\n",
            "                   all         46         53      0.392      0.571        0.5      0.248\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/29      3.17G      1.264      1.786      1.656          1        416: 100% 30/30 [00:08<00:00,  3.38it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.47it/s]\n",
            "                   all         46         53      0.445      0.541      0.447      0.233\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/29      3.17G       1.29      1.858      1.634          1        416: 100% 30/30 [00:06<00:00,  4.66it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.74it/s]\n",
            "                   all         46         53      0.392       0.58      0.446      0.252\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/29      3.17G      1.284      1.826      1.634          1        416: 100% 30/30 [00:09<00:00,  3.11it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.64it/s]\n",
            "                   all         46         53      0.472       0.63      0.662      0.377\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/29      3.17G       1.26      1.777      1.714          1        416: 100% 30/30 [00:09<00:00,  3.22it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.68it/s]\n",
            "                   all         46         53      0.624      0.556       0.61      0.345\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      21/29      3.17G      1.227       1.83      1.642          1        416: 100% 30/30 [00:06<00:00,  4.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.72it/s]\n",
            "                   all         46         53      0.737      0.502      0.617      0.347\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      22/29      3.17G      1.203      1.617      1.606          1        416: 100% 30/30 [00:09<00:00,  3.10it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.69it/s]\n",
            "                   all         46         53       0.76       0.56      0.655      0.386\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      23/29      3.17G      1.162      1.656      1.561          2        416: 100% 30/30 [00:09<00:00,  3.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.67it/s]\n",
            "                   all         46         53      0.664      0.582      0.615       0.36\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      24/29      3.17G      1.254       1.55      1.618          1        416: 100% 30/30 [00:06<00:00,  4.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  3.49it/s]\n",
            "                   all         46         53      0.483      0.659      0.655      0.388\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      25/29      3.17G      1.152      1.555      1.553          1        416: 100% 30/30 [00:07<00:00,  3.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.72it/s]\n",
            "                   all         46         53      0.779      0.629      0.677      0.385\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      26/29      3.17G      1.151      1.393      1.572          1        416: 100% 30/30 [00:07<00:00,  4.01it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  3.20it/s]\n",
            "                   all         46         53      0.746      0.648      0.682      0.387\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      27/29      3.17G      1.168      1.455      1.556          2        416: 100% 30/30 [00:08<00:00,  3.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.28it/s]\n",
            "                   all         46         53      0.559      0.607      0.653      0.373\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      28/29      3.17G       1.18      1.471      1.555          1        416: 100% 30/30 [00:06<00:00,  4.65it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.66it/s]\n",
            "                   all         46         53      0.564      0.615      0.637      0.368\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      29/29      3.17G      1.061      1.278      1.448          1        416: 100% 30/30 [00:09<00:00,  3.12it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.65it/s]\n",
            "                   all         46         53      0.598       0.64      0.634      0.374\n",
            "\n",
            "30 epochs completed in 0.106 hours.\n",
            "Optimizer stripped from runs/train/exp3/weights/last.pt, saved as runs/train/exp3/weights/last_striped.pt, 51.4MB\n",
            "Optimizer stripped from runs/train/exp3/weights/best.pt, saved as runs/train/exp3/weights/best_striped.pt, 51.4MB\n",
            "\n",
            "Validating runs/train/exp3/weights/best.pt...\n",
            "Fusing layers... \n",
            "gelan-c summary: 467 layers, 25417128 parameters, 0 gradients, 102.5 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.44it/s]\n",
            "                   all         46         53      0.746      0.648      0.683      0.387\n",
            "         JAAL-PILU-DAY         46          6      0.855          1      0.972      0.721\n",
            "         Kanar-farview         46          5      0.484        0.4      0.346      0.183\n",
            "        Kanar-nearview         46          9      0.975          1      0.995      0.621\n",
            "        NEEM-Near-view         46          8      0.613      0.875      0.931      0.462\n",
            "         NEEM-far-view         46          3      0.381      0.254      0.399      0.255\n",
            "       Pilkhan-Damaged         46          9      0.771      0.778      0.855      0.309\n",
            "       Pilkhan-Farview         46          8      0.888      0.875      0.949      0.543\n",
            "      Pilkhan-Nearview         46          5          1          0     0.0136    0.00353\n",
            "Results saved to \u001b[1mruns/train/exp3\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examine Training Results"
      ],
      "metadata": {
        "id": "fpCwjSUg2Mrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/yolov9/runs/train/exp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WslwgMAW2Euc",
        "outputId": "51128534-536c-4cfd-ea43-011826ed363b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion_matrix.png\t\t\t\t    opt.yaml\t  train_batch0.jpg\n",
            "events.out.tfevents.1716524641.af5ca9261918.1006.0  P_curve.png   train_batch1.jpg\n",
            "F1_curve.png\t\t\t\t\t    PR_curve.png  train_batch2.jpg\n",
            "hyp.yaml\t\t\t\t\t    R_curve.png   val_batch0_labels.jpg\n",
            "labels_correlogram.jpg\t\t\t\t    results.csv   val_batch0_pred.jpg\n",
            "labels.jpg\t\t\t\t\t    results.png   weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(filename=f\"/content/yolov9/runs/train/exp6/results.png\", width=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "grirpuCstpZE",
        "outputId": "4c2ac568-f42f-4836-d672-8a65b83389fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/yolov9/runs/train/exp6/results.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2e8ce18e574b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"/content/yolov9/runs/train/exp6/results.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1232\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/yolov9/runs/train/exp6/results.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(filename=f\"/content/yolov9/runs/train/exp6/confusion_matrix.png\", width=1000)"
      ],
      "metadata": {
        "id": "qggEg7Hv1zJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(filename=f\"/content/yolov9/runs/train/exp6/val_batch0_pred.jpg\", width=1000)"
      ],
      "metadata": {
        "id": "Xja2fjTl32Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 detect.py --/content/yolov9/{HOME}/weights/gelan-c.pt --source 2 --device cpu"
      ],
      "metadata": {
        "id": "XRHVVNrd8Ojl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate Custom Model"
      ],
      "metadata": {
        "id": "ih1rk9O_4CYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/yolov9/\n",
        "\n",
        "!python val.py \\\n",
        "--img 640 --batch 32 --conf 0.001 --iou 0.7 --device 0 \\\n",
        "--data /content/yolov9/yolov9-1/data.yaml \\\n",
        "--weights /content/yolov9/runs/train/exp6/weights/best.pt"
      ],
      "metadata": {
        "id": "XoZv8kNE4NxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with Custom Model"
      ],
      "metadata": {
        "id": "qJJ5fiqT6mEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py \\\n",
        "--img 1280 --conf 0.1 --device 0 \\\n",
        "--weights /content/yolov9/runs/train/exp6/weights/best.pt \\\n",
        "--source /content/yolov9/yolov9-1/test/images"
      ],
      "metadata": {
        "id": "8vnrn9cwIsUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Just like behore, the inference results have been saved in the appropriate directory inside `{HOME}/yolov9/runs/detect/`. Let's examine few of those results."
      ],
      "metadata": {
        "id": "WPbhTtVXtM4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for image_path in glob.glob(f'/content/yolov9/runs/detect/exp2/*.jpg')[:2]:\n",
        "      display(Image(filename=image_path, width=600))"
      ],
      "metadata": {
        "id": "XoV4sGOKJPZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BONUS: Deploy YOLOv9 Model with Inference"
      ],
      "metadata": {
        "id": "EMTTVZU48DdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** To deploy the model and display inference results, we will need two additional packages - [`inference`](https://pypi.org/project/inference) and [`supervision`](https://pypi.org/project/supervision). Let's install and import them!"
      ],
      "metadata": {
        "id": "QoDQAk5arRfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q inference supervision"
      ],
      "metadata": {
        "id": "Xn6YWeaa8bdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import random\n",
        "import getpass\n",
        "\n",
        "import supervision as sv\n",
        "\n",
        "from inference import get_model\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "4BauaNyA8wrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Before using your model in Inference, you first need to upload your weights to Roboflow Universe. Ensure to specify the correct `model_type` - `yolov9`, and that the project version matches the version of the dataset you used for training, denoted by `[1]`. In my case, it's `6`.\n",
        "\n",
        "![YOLOv9 Benchmark](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/upload-roboflow-model.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "wu0-mgYpskPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "version.deploy(model_type=\"yolov9\", model_path=f\"content/yolov9/runs/train/exp2\")"
      ],
      "metadata": {
        "id": "tV-BnNU-7_4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Now we can download our model anywhere using the assigned `model_id` denoted by `[2]`. In my case `football-players-detection-3zvbc/6`. To download the model you will need your [`ROBOFLOW_API_KEY`](https://docs.roboflow.com/api-reference/authentication).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KH30xwvAx1nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROBOFLOW_API_KEY = getpass.getpass()\n",
        "\n",
        "model = get_model(model_id=\"football-players-detection-3zvbc/8\", api_key=ROBOFLOW_API_KEY)"
      ],
      "metadata": {
        "id": "bAB-5ZMM87w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Let's pick random image from our test subset and detect objects using newly fine-tuned model."
      ],
      "metadata": {
        "id": "5pGSLZ8Fz5qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = sv.list_files_with_extensions(\n",
        "    directory=f\"{dataset.location}/test/images\",\n",
        "    extensions=['png', 'jpg', 'jpeg']\n",
        ")\n",
        "image_path = random.choice(image_paths)\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "result = model.infer(image, confidence=0.1)[0]\n",
        "detections = sv.Detections.from_inference(result)"
      ],
      "metadata": {
        "id": "Aes2oRxi9Kpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Finally, let's use supervision and [annotate](https://supervision.roboflow.com/develop/annotators/) our results."
      ],
      "metadata": {
        "id": "j8Xdr3Vp1uir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_annotator = sv.LabelAnnotator(text_color=sv.Color.BLACK)\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
        "\n",
        "annotated_image = image.copy()\n",
        "annotated_image = bounding_box_annotator.annotate(scene=annotated_image, detections=detections)\n",
        "annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n",
        "\n",
        "sv.plot_image(annotated_image)"
      ],
      "metadata": {
        "id": "Kq0BKx1_-kAy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}